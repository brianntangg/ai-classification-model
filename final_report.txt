BINARY IMAGE CLASSIFICATION: PEOPLE VS FOOD
=============================================
Final Project Report


1. INTRODUCTION

For this project I built a binary image classifier that can tell apart
photos of people from photos of food. The model is a convolutional neural
network written from scratch in PyTorch, trained on a small dataset of
64 images that I collected myself. The idea was to go through the full
pipeline -- collecting data, preprocessing, building a model, training it,
and then seeing how well it actually performs on images it hasn't seen.


2. DATASET

I collected 64 JPEG images total, split evenly between people and food
(32 each). These are real photos from personal photo libraries, so they
have natural variation in lighting, angles, and backgrounds.

I organized them into train, validation, and test folders so PyTorch's
ImageFolder loader could pick them up automatically:

  Split         People    Food    Total    Proportion
  ---------------------------------------------------
  Training        25       25       50       78%
  Validation       2        2        4        6%
  Test             5        5       10       16%
  ---------------------------------------------------
  Total           32       32       64      100%

Both classes are balanced in every split. ImageFolder assigns indices
alphabetically, so food = 0 and people = 1.


3. PREPROCESSING

Every image gets resized to 64x64 pixels and normalized to a [-1, 1]
range (mean=0.5, std=0.5 per channel). This gives the model a consistent
input size and centers the pixel values around zero, which helps with
training.

For the training set I also added some augmentation:
  - Random horizontal flips (50% chance)
  - Small random rotations (up to 10 degrees)
  - Color jitter on brightness and contrast (up to 20%)

The validation and test sets only get resized and normalized -- no
augmentation -- so the accuracy numbers reflect how the model does on
unmodified images.


4. MODEL ARCHITECTURE

The model (PeopleFoodCNN) has three convolutional blocks followed by
two fully connected layers.

Each block does: 3x3 convolution -> batch normalization -> ReLU -> 2x2
max pooling. The channels go from 3 (RGB) to 16 to 32 to 64, while the
spatial size gets halved each time (64 -> 32 -> 16 -> 8). After the last
block the feature maps are 64x8x8 = 4096 values, which get flattened and
fed into a fully connected layer with 128 units, then dropout at 30%,
and finally a 2-unit output layer.

  Layer                          Output Shape         Parameters
  ---------------------------------------------------------------
  Conv2d(3 -> 16, 3x3, pad=1)   (B, 16, 64, 64)          448
  BatchNorm2d(16)                (B, 16, 64, 64)           32
  ReLU + MaxPool2d(2x2)         (B, 16, 32, 32)            -
  Conv2d(16 -> 32, 3x3, pad=1)  (B, 32, 32, 32)        4,640
  BatchNorm2d(32)                (B, 32, 32, 32)           64
  ReLU + MaxPool2d(2x2)         (B, 32, 16, 16)            -
  Conv2d(32 -> 64, 3x3, pad=1)  (B, 64, 16, 16)       18,496
  BatchNorm2d(64)                (B, 64, 16, 16)          128
  ReLU + MaxPool2d(2x2)         (B, 64, 8, 8)              -
  Flatten                        (B, 4096)                  -
  Linear(4096 -> 128)            (B, 128)             524,416
  ReLU + Dropout(0.3)            (B, 128)                   -
  Linear(128 -> 2)               (B, 2)                   258
  ---------------------------------------------------------------
  Total trainable parameters: 548,482

The batch norm helps keep training stable, and the dropout is there to
reduce overfitting by randomly turning off neurons during training. The
output is two logits that go into CrossEntropyLoss, which handles the
softmax internally.


5. TRAINING CONFIGURATION

  Optimizer:       Adam (lr = 0.001)
  Loss function:   CrossEntropyLoss
  Batch size:      8
  Epochs:          30
  Device:          CPU

I went with Adam because it generally converges faster than plain SGD,
especially with smaller datasets. The batch size of 8 was small enough
to fit easily in memory and also adds a bit of noise to the gradients
which can help with generalization. I trained for 30 epochs, which was
enough time for the loss to flatten out.


6. RESULTS

Here are the per-epoch numbers:

  Epoch   Train Loss   Val Acc (%)   Test Acc (%)
  -------------------------------------------------
      1       1.0470       100.00          80.00
      2       1.0916        50.00          50.00
      3       0.6343       100.00         100.00
      4       0.6055       100.00          70.00
      5       0.3877       100.00          80.00
      6       0.4191       100.00          90.00
      7       0.2961       100.00          90.00
      8       0.2527       100.00         100.00
      9       0.1438       100.00          80.00
     10       0.1663       100.00          80.00
     11       0.0731       100.00          90.00
     12       0.1337       100.00          80.00
     13       0.0891       100.00          90.00
     14       0.0917       100.00          80.00
     15       0.1087       100.00          80.00
     16       0.2843       100.00          80.00
     17       0.1410       100.00         100.00
     18       0.2248       100.00          80.00
     19       0.1252       100.00          80.00
     20       0.1467       100.00          80.00
     21       0.0709       100.00          60.00
     22       0.0342       100.00          80.00
     23       0.0929       100.00          80.00
     24       0.1129       100.00          80.00
     25       0.0349       100.00          80.00
     26       0.0634       100.00          80.00
     27       0.0112       100.00          80.00
     28       0.0160       100.00          80.00
     29       0.0415       100.00          80.00
     30       0.0154       100.00          80.00
  -------------------------------------------------

  Final test accuracy:       80.00%
  Best test accuracy:       100.00% (epoch 3)
  Final validation accuracy: 100.00%
  Final training loss:        0.0154

The training loss dropped from about 1.05 down to 0.015 over the 30
epochs. It was a bit jumpy in the first 10 epochs or so but smoothed
out after that.

Validation accuracy stayed at 100% almost the entire time. That sounds
great but the validation set is only 4 images, so getting even one wrong
drops it to 75%. It's too small to really tell us much.

Test accuracy bounced around quite a bit -- anywhere from 60% to 100%
depending on the epoch. It hit 100% a few times (epochs 3, 8, 17) but
mostly sat at 80%. By the end of training it settled at 80%, meaning
2 out of 10 test images were being misclassified.


7. DISCUSSION

80% on the test set means the model got 8 out of 10 right. That's decent
considering how small the dataset is, but there's clearly room for
improvement.

The training loss got very close to zero, which tells me the model
basically memorized the training data. With 548,000 parameters and only
50 training images, that's not surprising. The gap between perfect
training accuracy and 80% test accuracy is a sign of overfitting -- the
model learned the training images really well but didn't fully generalize
to new ones.

The 2 images it kept getting wrong are probably tricky cases. Some of
the photos in the dataset have people near food or food in the background
of people shots, which could easily confuse the model. With only 10 test
images, even one borderline photo has a big impact on the accuracy number.

The validation set at 4 images is really too small to be useful as a
guide during training. It mostly just confirmed that those 4 particular
images were easy for the model.


8. LIMITATIONS AND IMPROVEMENTS

The biggest limitation is the dataset size. 64 images is very small for
training a CNN -- normally you'd want hundreds or thousands per class.
The model has way more parameters than training examples, which makes
overfitting hard to avoid.

The 64x64 input resolution also throws away a lot of detail. At that
size, faces become blurry and smaller features in food images get lost.
Bumping this up to 128x128 or 224x224 would probably help.

If I were to improve this project, the two biggest things would be:
getting more training data (at least a couple hundred images per class),
and trying transfer learning with a pretrained model like ResNet-18
instead of training from scratch. A pretrained model already knows how
to extract useful features from images, so it would likely do much
better even with limited data. Adding a learning rate scheduler could
also help the model converge more precisely toward the end of training.


9. FILES

  train.py                       Training script
  final_report.txt               This report
  data/                          Dataset (train/val/test splits)
  outputs/model.pth              Trained model weights
  outputs/training_loss.png      Training loss plot
  outputs/test_accuracy.png      Test accuracy plot
  outputs/training_log.txt       Per-epoch training log


10. CONCLUSION

This project covered the full process of building an image classifier
in PyTorch: collecting images, preprocessing them, designing a CNN,
training it, and evaluating the results. The model ended up at 80% test
accuracy, which is reasonable for 50 training images but not great
overall. The main takeaway is that dataset size matters a lot -- with
more data or a pretrained model, the accuracy would almost certainly
be higher.
